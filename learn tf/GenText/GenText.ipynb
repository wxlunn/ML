{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dd8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f41cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483d0956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac5b9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63107d3",
   "metadata": {},
   "source": [
    "# Processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f367df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits into tokens and converts characters to id\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "# function that converts id to chracters\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4acd95",
   "metadata": {},
   "outputs": [],
   "source": [
    " # function that joins characters back into strings\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ab5954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 48, 57, ..., 46,  9,  1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splits the dataset and converts characters to id\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')).numpy()\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018b33db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a9978f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "# sequence and label\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de627b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits sequences into input and target\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27166fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0c9c0",
   "metadata": {},
   "source": [
    "# Create RNN Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5caf063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "print(vocab_size)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d7e8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to specify the length of the sequences, model can accept any length\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4428abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8323dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "tf.Tensor(\n",
      "[[-1.9334422  -0.3488962   1.4828081  ... -0.8846549   1.5826918\n",
      "  -1.1738349 ]\n",
      " [-2.3274357  -1.9890401  -0.5178334  ... -0.59782416  0.8016198\n",
      "   0.74277264]\n",
      " [-4.602709    1.8313886   3.973525   ... -1.9603542   1.1835002\n",
      "  -1.73887   ]\n",
      " ...\n",
      " [-5.6212564  -0.48659518  1.4783057  ... -2.4719584   1.0856739\n",
      "  -0.26215833]\n",
      " [-5.706069    2.0034118   4.6389537  ... -3.4942462   0.5997738\n",
      "  -2.5121825 ]\n",
      " [-7.070782    2.3340266   5.3439507  ... -4.03253     1.0826193\n",
      "  -3.774559  ]], shape=(100, 66), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-7.0707822e+00  2.3340266e+00  5.3439507e+00 ... -4.0325298e+00\n",
      "   1.0826193e+00 -3.7745590e+00]\n",
      " [-5.8076353e+00 -1.4261122e+00  3.2631416e+00 ... -1.3284305e+00\n",
      "   3.6629736e+00 -3.4907199e-03]\n",
      " [-4.3919621e+00 -3.5034957e+00  1.2084309e-01 ... -4.9032840e-01\n",
      "   2.1410909e+00  5.5546522e-01]\n",
      " ...\n",
      " [-4.3845778e+00 -3.6106577e+00 -3.9978757e+00 ... -3.4689653e+00\n",
      "   1.6002932e+00 -2.7677622e+00]\n",
      " [-4.7972298e+00  3.0695508e+00 -2.3223028e+00 ... -3.9440885e+00\n",
      "   1.8005838e-01 -4.0025702e+00]\n",
      " [-5.4598260e+00 -2.8278956e-01  5.1002784e+00 ... -1.5558115e+00\n",
      "   1.4228745e+00 -8.3398163e-01]], shape=(64, 66), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(example_batch_predictions[0])\n",
    "    \n",
    "example_batch_predictions[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb886579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_3 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6615a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65  4 63 52 50 39 13 16 44 28 44  5 15 41 31 10 16 56 35 17 25  4 19 51\n",
      "  2 48 31 43 20 43 21  1 33  9 44 48 65 21  3 59 17 45 52 10  0  4 15 40\n",
      " 28 19 23 58 27 38 59 13  2 32 45 17 16 39 62 57  0 49 20 10 15 57 13 21\n",
      " 52 36 36 12 28 20 16  7 28 35 31 37 39 24 24 38 47 10 46 46 50 57 39 28\n",
      " 41 58 55 28]\n"
     ]
    }
   ],
   "source": [
    "# random sample with the biggest chance \n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "#print(sampled_indices)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf858c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'ou within the compass of my curse.\\n\\nBUCKINGHAM:\\nNor no one here; for curses never pass\\nThe lips of t'\n",
      "Next Char Predictions:\n",
      " b'z$xmkZ?CeOe&BbR3CqVDL$Fl iRdGdH\\nT.eizH!tDfm3[UNK]$BaOFJsNYt? SfDCZwr[UNK]jG3Br?HmWW;OGC,OVRXZKKYh3ggkrZObspO'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95658967",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0bd5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b78f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1903133, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d29614c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.04348"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebbf8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f518caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "246286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eabcf999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 207s 1s/step - loss: 2.7290\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b9ba263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor([[0]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(ids_from_chars(['[UNK]']))\n",
    "print(ids_from_chars(['[UNK]'])[:, None])\n",
    "skip_ids = ids_from_chars(['[UNK]'])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35ac6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ca7e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba0a21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Sow ile ars aling fion, Ald inie de for,sesses toour\n",
      "Amun', wok' tik noscee.\n",
      "Peare. Wher, thay, lot hiontly geallim;\n",
      "Tors hithes ou nager hat\n",
      "Th bavo, not\n",
      "Aly soo telis foubnous bfiy son,\n",
      "Beve profne an if l,\n",
      "Bury heapedod has, wa menge you wor cickain, for stimeze, sour dith:\n",
      "Thath of the mor.\n",
      "\n",
      "PORLISAE:\n",
      "Wite sesar,e?\n",
      "\n",
      "Pod apang of yonqust moide\n",
      "Maftunn\n",
      "whlond iod me't my hat meds wime?\n",
      "\n",
      "BQCCELTUNFO:\n",
      "An to dom?\n",
      "\n",
      "MENGE:\n",
      "Mamvang of he lige.\n",
      "\n",
      "ThRYLIRCEN:\n",
      "I Tocd, Io gorur,.\n",
      "\n",
      "RUBERTOCDO:\n",
      "I aigh\n",
      "Ce marche of att ie hend abrveave har of lusd hous on to gor it menge rot to dow lat meancho,\n",
      "West ald chead an int Room buinstis neve lawes: do for lot's, ica she ane refo\n",
      "tate pricithe', whing out co mary thy byous thows ape keroo angu; wo lesan'g,\n",
      "Trets:\n",
      "The beere\n",
      "Thath the wist of youter hay orbus\n",
      "Jur, wive hove,\n",
      "Yy, Goknd com mind shour:\n",
      "Or shard thocisin then tom of tod irign's hisge.\n",
      "\n",
      "ABMIRY:\n",
      "Fith beaving.\n",
      "\n",
      "PmIA,\n",
      "tho Whake:\n",
      "Nopdange,\n",
      "Af wir.\n",
      "\n",
      "CINHINI HINI:\n",
      "Aped at thge tis upelts.\n",
      "\n",
      "HONDIIU\n",
      "A \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.3190343379974365\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aee1ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000002740EB3A550>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a96ca883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Hes pe om uld foth maca.\n",
      "\n",
      "TANDs Evy, Oadthed frougin myour the aud thot in suomst. IBant thesk you \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ed020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
